\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\date{}

\begin{document}

\textbf{Hidden Markov Models}

Pushkin Rathore 18111043

15\textsuperscript{th} September, 2021

The HMM is based on augmenting the Markov chain. A Markov chain is a
model that tells us something about the probabilities of sequences of
random variables, states, each of which can take on values from some
set. These sets can be words, or tags, or symbols representing anything,
like the weather. A Markov chain makes a very strong assumption that if
we want to predict the future in the sequence, all that matters is the
current state. The states before the current state have no impact on the
future except via the current state. It's as if to predict tomorrow's
weather you could examine today's weather but you weren't allowed to
look at yesterday's weather.

\includegraphics[width=3.31200in,height=2.07917in]{media/image1.JPG}

\emph{A Markov chain for weather (a), showing states and transitions. A
start distribution π is required; setting π = {[}0.1, 0.7, 0.2{]} for
(a) would mean a probability 0.7 of starting in state 2 (cold),
probability 0.1 of starting in state 1 (hot), etc.}

More formally, consider a sequence of state variables q1, q2,
\ldots{}... qi. A Markov model embodies the Markov assumption on the
probabilities of this sequence: that Markov assumption when predicting
the future, the past doesn't matter, only the present. Figure (a) shows
a Markov chain for assigning a probability to a sequence of weather
events, for which the vocabulary consists of HOT, COLD, and WARM. The
states are represented as nodes in the graph, and the transitions, with
their probabilities, as edges. The transitions are probabilities: the
values of arcs leaving a given state must sum to 1.

The reason it is called a Hidden Markov Model is because we are
constructing an inference model based on the assumptions of a Markov
process. The Markov process assumption is simply that the ``future is
independent of the past given the present''. In other words, assuming we
know our present state, we do not need any other historical information
to predict the future state. To make this point clear, let us consider
the scenario below where the weather, the hidden variable, can be hot,
mild or cold and the observed variables are the type of clothing worn.
The arrows represent transitions from a hidden state to another hidden
state or from a hidden state to an observed variable. Notice that, true
to the Markov assumption, each state only depends on the previous state
and not on any other prior states.

Markov and Hidden Markov models are engineered to handle data which can
be represented as `sequence' of observations over time. Hidden Markov
models are probabilistic frameworks where the observed data are modelled
as a series of outputs generated by one of several (hidden) internal
states. Markov models are developed based on mainly two assumptions.

\textbf{Limited Horizon assumption}: Probability of being in a state at
a time t depend only on the state at the time (t-1).

\includegraphics[width=2.53044in,height=0.40506in]{media/image2.png}

That means state at time t represents enough summary of the past
reasonably to predict the future. This assumption is an Order-1 Markov
process. An order-k Markov process assumes conditional independence of
state z\textsubscript{t} from the states that are k + 1-time steps
before it.

\textbf{Stationary Process Assumption}: Conditional (probability)
distribution over the next state, given the current state, doesn't
change over time.

\includegraphics[width=3.11940in,height=0.26866in]{media/image3.png}

That means states keep on changing over time but the underlying process
is stationary. the Markov property specifies that the probability of a
state depends only on the probability of the previous state, but we can
build more ``memory'' into our states by using a higher order Markov
model. In an nth order Markov model:

\includegraphics[width=3.36567in,height=0.32619in]{media/image4.png}

\end{document}
